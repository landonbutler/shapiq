{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b000618e37afdb3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Explaining a language model for sentiment analysis\n",
    "\n",
    "This notebook shows how we can use `shapiq` to explain the predictions of a language sentiment analysis model. For that, we will create a custom *game* that will be used for the explanation. The benchmark game resulting from this tutorial is available as `shapiq.games.SentimentClassificationGame`.\n",
    "\n",
    "First, we need to install the required packages next to `shapiq`. We will use a language model from the `transformers` library; specifically relying on `torch`."
   ]
  },
  {
   "cell_type": "code",
   "id": "96756a5298128aed",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-09T21:35:24.724377Z",
     "start_time": "2025-09-09T21:35:23.385912Z"
    }
   },
   "source": [
    "# Install the required packages\n",
    "!pip install transformers torch"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/landonb/PycharmProjects/SparseInference/venv/lib/python3.11/site-packages (4.56.1)\r\n",
      "Requirement already satisfied: torch in /Users/landonb/PycharmProjects/SparseInference/venv/lib/python3.11/site-packages (2.8.0)\r\n",
      "Requirement already satisfied: filelock in /Users/landonb/PycharmProjects/SparseInference/venv/lib/python3.11/site-packages (from transformers) (3.19.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/landonb/PycharmProjects/SparseInference/venv/lib/python3.11/site-packages (from transformers) (0.34.4)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/landonb/PycharmProjects/SparseInference/venv/lib/python3.11/site-packages (from transformers) (2.2.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/landonb/PycharmProjects/SparseInference/venv/lib/python3.11/site-packages (from transformers) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/landonb/PycharmProjects/SparseInference/venv/lib/python3.11/site-packages (from transformers) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/landonb/PycharmProjects/SparseInference/venv/lib/python3.11/site-packages (from transformers) (2025.9.1)\r\n",
      "Requirement already satisfied: requests in /Users/landonb/PycharmProjects/SparseInference/venv/lib/python3.11/site-packages (from transformers) (2.32.5)\r\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/landonb/PycharmProjects/SparseInference/venv/lib/python3.11/site-packages (from transformers) (0.22.0)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/landonb/PycharmProjects/SparseInference/venv/lib/python3.11/site-packages (from transformers) (0.6.2)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/landonb/PycharmProjects/SparseInference/venv/lib/python3.11/site-packages (from transformers) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/landonb/PycharmProjects/SparseInference/venv/lib/python3.11/site-packages (from torch) (4.15.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/landonb/PycharmProjects/SparseInference/venv/lib/python3.11/site-packages (from torch) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in /Users/landonb/PycharmProjects/SparseInference/venv/lib/python3.11/site-packages (from torch) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /Users/landonb/PycharmProjects/SparseInference/venv/lib/python3.11/site-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /Users/landonb/PycharmProjects/SparseInference/venv/lib/python3.11/site-packages (from torch) (2025.9.0)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/landonb/PycharmProjects/SparseInference/venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/landonb/PycharmProjects/SparseInference/venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/landonb/PycharmProjects/SparseInference/venv/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/landonb/PycharmProjects/SparseInference/venv/lib/python3.11/site-packages (from requests->transformers) (3.4.3)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/landonb/PycharmProjects/SparseInference/venv/lib/python3.11/site-packages (from requests->transformers) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/landonb/PycharmProjects/SparseInference/venv/lib/python3.11/site-packages (from requests->transformers) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/landonb/PycharmProjects/SparseInference/venv/lib/python3.11/site-packages (from requests->transformers) (2025.8.3)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "233a68eadd33ade3",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-09T21:35:31.420084Z",
     "start_time": "2025-09-09T21:35:24.731343Z"
    }
   },
   "source": [
    "# Import the required libraries\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "\n",
    "import shapiq\n",
    "\n",
    "print(f\"shapiq version: {shapiq.__version__}\")"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas._config'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnp\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtransformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m pipeline\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mshapiq\u001B[39;00m\n\u001B[32m      7\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mshapiq version: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mshapiq.__version__\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/SparseInference/venv/lib/python3.11/site-packages/shapiq/__init__.py:28\u001B[39m\n\u001B[32m      9\u001B[39m \u001B[38;5;66;03m# approximator classes\u001B[39;00m\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mapproximator\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m     11\u001B[39m     SHAPIQ,\n\u001B[32m     12\u001B[39m     SPEX,\n\u001B[32m   (...)\u001B[39m\u001B[32m     26\u001B[39m     kADDSHAP,\n\u001B[32m     27\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m28\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mbenchmark\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m     29\u001B[39m     BENCHMARK_CONFIGURATIONS,\n\u001B[32m     30\u001B[39m     GAME_CLASS_TO_NAME_MAPPING,\n\u001B[32m     31\u001B[39m     GAME_NAME_TO_CLASS_MAPPING,\n\u001B[32m     32\u001B[39m     download_game_data,\n\u001B[32m     33\u001B[39m     load_benchmark_results,\n\u001B[32m     34\u001B[39m     load_game_data,\n\u001B[32m     35\u001B[39m     load_games_from_configuration,\n\u001B[32m     36\u001B[39m     plot_approximation_quality,\n\u001B[32m     37\u001B[39m     print_benchmark_configurations,\n\u001B[32m     38\u001B[39m     run_benchmark,\n\u001B[32m     39\u001B[39m     run_benchmark_from_configuration,\n\u001B[32m     40\u001B[39m )\n\u001B[32m     42\u001B[39m \u001B[38;5;66;03m# dataset functions\u001B[39;00m\n\u001B[32m     43\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdatasets\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m load_adult_census, load_bike_sharing, load_california_housing\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/SparseInference/venv/lib/python3.11/site-packages/shapiq/benchmark/__init__.py:3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[33;03m\"\"\"This module contains all functions for conducting benchmarks with the SHAPIQ package.\"\"\"\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mconfiguration\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m      4\u001B[39m     BENCHMARK_CONFIGURATIONS,\n\u001B[32m      5\u001B[39m     GAME_CLASS_TO_NAME_MAPPING,\n\u001B[32m      6\u001B[39m     GAME_NAME_TO_CLASS_MAPPING,\n\u001B[32m      7\u001B[39m     print_benchmark_configurations,\n\u001B[32m      8\u001B[39m )\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mload\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m download_game_data, load_game_data, load_games_from_configuration\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mplot\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m plot_approximation_quality\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/SparseInference/venv/lib/python3.11/site-packages/shapiq/benchmark/configuration.py:49\u001B[39m\n\u001B[32m     39\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtyping\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m TYPE_CHECKING, Any\n\u001B[32m     41\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mshapiq\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mapproximator\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m     42\u001B[39m     FSII_APPROXIMATORS,\n\u001B[32m     43\u001B[39m     SI_APPROXIMATORS,\n\u001B[32m   (...)\u001B[39m\u001B[32m     47\u001B[39m     Approximator,\n\u001B[32m     48\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m49\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mshapiq\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mgames\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mbenchmark\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m     50\u001B[39m     SOUM,\n\u001B[32m     51\u001B[39m     AdultCensusDatasetValuation,\n\u001B[32m     52\u001B[39m     AdultCensusDataValuation,\n\u001B[32m     53\u001B[39m     AdultCensusEnsembleSelection,\n\u001B[32m     54\u001B[39m     AdultCensusFeatureSelection,\n\u001B[32m     55\u001B[39m     AdultCensusGlobalXAI,\n\u001B[32m     56\u001B[39m     AdultCensusLocalXAI,\n\u001B[32m     57\u001B[39m     AdultCensusRandomForestEnsembleSelection,\n\u001B[32m     58\u001B[39m     AdultCensusUncertaintyExplanation,\n\u001B[32m     59\u001B[39m     AdultCensusUnsupervisedData,\n\u001B[32m     60\u001B[39m     BikeSharingClusterExplanation,\n\u001B[32m     61\u001B[39m     BikeSharingDatasetValuation,\n\u001B[32m     62\u001B[39m     BikeSharingDataValuation,\n\u001B[32m     63\u001B[39m     BikeSharingEnsembleSelection,\n\u001B[32m     64\u001B[39m     BikeSharingFeatureSelection,\n\u001B[32m     65\u001B[39m     BikeSharingGlobalXAI,\n\u001B[32m     66\u001B[39m     BikeSharingLocalXAI,\n\u001B[32m     67\u001B[39m     BikeSharingRandomForestEnsembleSelection,\n\u001B[32m     68\u001B[39m     BikeSharingUnsupervisedData,\n\u001B[32m     69\u001B[39m     CaliforniaHousingClusterExplanation,\n\u001B[32m     70\u001B[39m     CaliforniaHousingDatasetValuation,\n\u001B[32m     71\u001B[39m     CaliforniaHousingDataValuation,\n\u001B[32m     72\u001B[39m     CaliforniaHousingEnsembleSelection,\n\u001B[32m     73\u001B[39m     CaliforniaHousingFeatureSelection,\n\u001B[32m     74\u001B[39m     CaliforniaHousingGlobalXAI,\n\u001B[32m     75\u001B[39m     CaliforniaHousingLocalXAI,\n\u001B[32m     76\u001B[39m     CaliforniaHousingRandomForestEnsembleSelection,\n\u001B[32m     77\u001B[39m     CaliforniaHousingUnsupervisedData,\n\u001B[32m     78\u001B[39m     ImageClassifierLocalXAI,\n\u001B[32m     79\u001B[39m     SentimentAnalysisLocalXAI,\n\u001B[32m     80\u001B[39m     \u001B[38;5;66;03m# not to be precomputed\u001B[39;00m\n\u001B[32m     81\u001B[39m     SynthDataTreeSHAPIQXAI,\n\u001B[32m     82\u001B[39m )\n\u001B[32m     84\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m TYPE_CHECKING:\n\u001B[32m     85\u001B[39m     \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mshapiq\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mgames\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mbase\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Game\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/SparseInference/venv/lib/python3.11/site-packages/shapiq/games/benchmark/__init__.py:4\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[33;03m\"\"\"This module contains all pre-defined benchmark games.\"\"\"\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# data and dataset valuation games\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdata_valuation\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mbase\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DataValuation\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdata_valuation\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mbenchmark\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m      6\u001B[39m     AdultCensus \u001B[38;5;28;01mas\u001B[39;00m AdultCensusDataValuation,\n\u001B[32m      7\u001B[39m     BikeSharing \u001B[38;5;28;01mas\u001B[39;00m BikeSharingDataValuation,\n\u001B[32m      8\u001B[39m     CaliforniaHousing \u001B[38;5;28;01mas\u001B[39;00m CaliforniaHousingDataValuation,\n\u001B[32m      9\u001B[39m )\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdataset_valuation\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mbase\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DatasetValuation\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/SparseInference/venv/lib/python3.11/site-packages/shapiq/games/benchmark/data_valuation/__init__.py:4\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[33;03m\"\"\"This module contains all dataset valuation benchmark games.\"\"\"\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mbase\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DataValuation\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mbenchmark\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m AdultCensus, BikeSharing, CaliforniaHousing\n\u001B[32m      6\u001B[39m __all__ = [\u001B[33m\"\u001B[39m\u001B[33mDataValuation\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mAdultCensus\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mBikeSharing\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mCaliforniaHousing\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/SparseInference/venv/lib/python3.11/site-packages/shapiq/games/benchmark/data_valuation/benchmark.py:6\u001B[39m\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m__future__\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m annotations\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mshapiq\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mgames\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mbenchmark\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdata_valuation\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mbase\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DataValuation\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mshapiq\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mgames\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mbenchmark\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01msetup\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m GameBenchmarkSetup\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mclass\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mCaliforniaHousing\u001B[39;00m(DataValuation):\n\u001B[32m     10\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"The California Housing dataset as a DataValuation game.\"\"\"\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/SparseInference/venv/lib/python3.11/site-packages/shapiq/games/benchmark/setup.py:24\u001B[39m\n\u001B[32m     21\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpreprocessing\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m StandardScaler\n\u001B[32m     22\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mtree\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DecisionTreeClassifier, DecisionTreeRegressor\n\u001B[32m---> \u001B[39m\u001B[32m24\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mshapiq\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdatasets\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m load_adult_census, load_bike_sharing, load_california_housing\n\u001B[32m     25\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mshapiq\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m shuffle_data\n\u001B[32m     27\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m TYPE_CHECKING:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/SparseInference/venv/lib/python3.11/site-packages/shapiq/datasets/__init__.py:3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[33;03m\"\"\"Datasets for testing and examples.\"\"\"\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_all\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m load_adult_census, load_bike_sharing, load_california_housing\n\u001B[32m      5\u001B[39m __all__ = [\u001B[33m\"\u001B[39m\u001B[33mload_bike_sharing\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mload_adult_census\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mload_california_housing\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/SparseInference/venv/lib/python3.11/site-packages/shapiq/datasets/_all.py:8\u001B[39m\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpathlib\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Path\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtyping\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m TYPE_CHECKING\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpandas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpd\u001B[39;00m\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m TYPE_CHECKING:\n\u001B[32m     11\u001B[39m     \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnp\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/SparseInference/venv/lib/python3.11/site-packages/pandas/__init__.py:37\u001B[39m\n\u001B[32m     30\u001B[39m     _module = _err.name\n\u001B[32m     31\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[32m     32\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mC extension: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m_module\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m not built. If you want to import \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     33\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mpandas from the source directory, you may need to run \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     34\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m'\u001B[39m\u001B[33mpython setup.py build_ext\u001B[39m\u001B[33m'\u001B[39m\u001B[33m to build the C extensions first.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     35\u001B[39m     ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m_err\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m37\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpandas\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_config\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m     38\u001B[39m     get_option,\n\u001B[32m     39\u001B[39m     set_option,\n\u001B[32m     40\u001B[39m     reset_option,\n\u001B[32m     41\u001B[39m     describe_option,\n\u001B[32m     42\u001B[39m     option_context,\n\u001B[32m     43\u001B[39m     options,\n\u001B[32m     44\u001B[39m )\n\u001B[32m     46\u001B[39m \u001B[38;5;66;03m# let init-time option registration happen\u001B[39;00m\n\u001B[32m     47\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpandas\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcore\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mconfig_init\u001B[39;00m  \u001B[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001B[39;00m\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'pandas._config'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "45f9a6a38b3b0214",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Language model\n",
    "We will use a pre-trained BERT model for sentiment analysis. We will use the `transformers` library to load the model and tokenizer. We will use the `lvwerra/distilbert-imdb` model for this tutorial.\n",
    "\n",
    "The model predicts the sentiment of the sentence as **positive**. For this model (and other sentiment-analysis models), the output is a list of dictionaries, where each dictionary contains the `label` and the `score` of the sentiment. The label can be either `POSITIVE` or `NEGATIVE`. The score is the probability of the sentiment being positive or negative. The tokenized sentence contains the tokens of the sentence. The special tokens map contains the special tokens used by the model. We will need the `mask_token` later in the game."
   ]
  },
  {
   "cell_type": "code",
   "id": "50f59cc77301eef0",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-09T21:34:13.563787Z",
     "start_time": "2025-09-09T21:34:11.128429Z"
    }
   },
   "source": [
    "# Load the model and tokenizer\n",
    "classifier = pipeline(task=\"sentiment-analysis\", model=\"lvwerra/distilbert-imdb\")\n",
    "tokenizer = classifier.tokenizer\n",
    "\n",
    "test_sentence = \"I love this movie!\"\n",
    "print(f\"Classifier output: {classifier(test_sentence)}\")\n",
    "\n",
    "tokenized_sentence = tokenizer(test_sentence)\n",
    "print(f\"Tokenized sentence: {tokenized_sentence}\")\n",
    "\n",
    "special_tokens = tokenizer.special_tokens_map\n",
    "print(f\"Special tokens: {tokenizer.special_tokens_map}\")\n",
    "\n",
    "mask_toke_id = tokenizer.mask_token_id\n",
    "print(f\"Mask token id: {mask_toke_id}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier output: [{'label': 'POSITIVE', 'score': 0.9655240178108215}]\n",
      "Tokenized sentence: {'input_ids': [101, 1045, 2293, 2023, 3185, 999, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n",
      "Special tokens: {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "Mask token id: 103\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "25e75bdac10f7042",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can inspect the behavior of the model by checking the output of the classifier for different sentences and by decoding the tokenized sentences. The `tokenizer.decode` function can be used to decode the tokenized sentence. The `[CLS]` token is used to mark the beginning of the sentence, and the `[SEP]` token is used to mark the end of the sentence. Notice that also the `!` token is tokenized."
   ]
  },
  {
   "cell_type": "code",
   "id": "c3b3b6f4193e7d73",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-09T21:34:16.950951Z",
     "start_time": "2025-09-09T21:34:16.948168Z"
    }
   },
   "source": [
    "# Test the tokenizer\n",
    "decoded_sentence = tokenizer.decode(tokenized_sentence[\"input_ids\"])\n",
    "print(f\"Decoded sentence: {decoded_sentence}\")\n",
    "\n",
    "# Remove the start and end tokens\n",
    "tokenized_input = np.asarray(tokenizer(test_sentence)[\"input_ids\"][1:-1])\n",
    "decoded_sentence = tokenizer.decode(tokenized_input)\n",
    "print(\n",
    "    f\"Decoded sentence: {decoded_sentence} - Tokenized input: {tokenized_input} - {len(tokenized_input)} tokens.\"\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded sentence: [CLS] i love this movie! [SEP]\n",
      "Decoded sentence: i love this movie! - Tokenized input: [1045 2293 2023 3185  999] - 5 tokens.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "97381c1da32a6c49",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Since the start and end tokens are always present this information is not relevant for our explanation. To explain this classifier we need to model its behavior as a cooperative game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca96f0af12688",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Treating the language model as a game with a value function\n",
    "For all Shapley-based feature attribution methods, we need to model the problem as a cooperative game. We need to define a **value function** that assigns a real-valued worth to each coalition of features. In this case, the features are the tokens of the sentence (without the `[CLS]` and `[SEP]` tokens). The value of the coalition is the sentiment score of the sentence with tokens that are not participating in the coalition `masked` or `removed`.\n",
    "\n",
    "A value function has the following formal definition:\n",
    "$$v: 2^N \\rightarrow \\mathbb{R}$$\n",
    "where $N$ is the set of features (tokens in our case). \n",
    "\n",
    "To be able to model `POSITIVE` and `NEGATIVE` sentiments, we need to map the output of the classifier to be in the range $[-1, 1]$. We can do this with the following function which accepts a list of input texts and returns a vector of the sentiment of the input texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "bce879ce457e9a98",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-09T21:34:20.279053Z",
     "start_time": "2025-09-09T21:34:20.219815Z"
    }
   },
   "source": [
    "# Define the model call function\n",
    "def model_call(input_texts: list[str]) -> np.ndarray[float]:\n",
    "    \"\"\"Calls the sentiment classification model with a list of texts.\n",
    "\n",
    "    Args:\n",
    "        input_texts: A list of input texts.\n",
    "\n",
    "    Returns:\n",
    "        A vector of the sentiment of the input texts.\n",
    "    \"\"\"\n",
    "    outputs = classifier(input_texts)\n",
    "    outputs = [\n",
    "        output[\"score\"] * 1 if output[\"label\"] == \"POSITIVE\" else output[\"score\"] * -1\n",
    "        for output in outputs\n",
    "    ]\n",
    "    return np.array(outputs, dtype=float)\n",
    "\n",
    "\n",
    "# Test the model call function\n",
    "print(f\"Model call: {model_call(['I love this movie!', 'I hate this movie!'])}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model call: [ 0.96552402 -0.90668219]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "ee183b3800498675",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "With this model call function, we can now define the value function. In our world the value function accepts one-hot-encoded numpy matrices denoting the coalitions."
   ]
  },
  {
   "cell_type": "code",
   "id": "d176905292347ec1",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-09T21:34:21.195582Z",
     "start_time": "2025-09-09T21:34:21.193308Z"
    }
   },
   "source": [
    "# Show coalitions\n",
    "n_players = len(tokenized_sentence[\"input_ids\"]) - 2  # remove [CLS] and [SEP]\n",
    "\n",
    "empty_coalition = np.zeros((1, n_players), dtype=bool)  # empty coalition\n",
    "full_coalition = np.ones((1, n_players), dtype=bool)  # full coalition\n",
    "\n",
    "print(f\"Empty coalition: {empty_coalition}\")\n",
    "print(f\"Full coalition: {full_coalition}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty coalition: [[False False False False False]]\n",
      "Full coalition: [[ True  True  True  True  True]]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "bb8100b1a3fc09e3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "With these coalitions we can now define the value function. However, for most algorithms it is important that the value function is normalized (also known as centered). This means that the value of the empty coalition is 0. We can achieve this by subtracting the value of the empty coalition from the value of the coalition. This is done in the `shapiq` library, but we can also do it here.\n",
    "\n",
    "Formally, the normalized value function is defined as:\n",
    "$$v_0 := v(S) - v(\\emptyset)$$\n",
    "where $v(S)$ is the value of the coalition $S$ and $v(\\emptyset)$ is the value of the empty coalition."
   ]
  },
  {
   "cell_type": "code",
   "id": "79a5c423622a0904",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-09T21:34:22.106993Z",
     "start_time": "2025-09-09T21:34:22.104361Z"
    }
   },
   "source": [
    "# Define the value function\n",
    "def value_function(\n",
    "    coalitions: np.ndarray[bool], tokenized_input: np.ndarray[int], normalization_value: float = 0.0\n",
    ") -> np.ndarray[float]:\n",
    "    \"\"\"Computes the value of the coalitions.\n",
    "\n",
    "    Args:\n",
    "        coalitions: A numpy matrix of shape (n_coalitions, n_players).\n",
    "        tokenized_input: A numpy array of the tokenized input sentence.\n",
    "        normalization_value: The value of the empty coalition. Default is 0.0 (no normalization).\n",
    "\n",
    "    Returns:\n",
    "        A vector of the value of the coalitions.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    for coalition in coalitions:\n",
    "        tokenized_coalition = tokenized_input.copy()\n",
    "        # all tokens not in the coalition are set to mask_token_id\n",
    "        tokenized_coalition[~coalition] = mask_toke_id\n",
    "        coalition_text = tokenizer.decode(tokenized_coalition)\n",
    "        texts.append(coalition_text)\n",
    "\n",
    "    # get the sentiment of the texts (call the model as defined above)\n",
    "    sentiments = model_call(texts)\n",
    "\n",
    "    # normalize/center the value function\n",
    "    return sentiments - normalization_value"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "a8b971656158325b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can test the value function without normalization. The output of the value function for the grand coalition (full coalition) should be the same as the output of the classifier. The output of the value function for the empty coalition is some bias value in the model which often is not zero."
   ]
  },
  {
   "cell_type": "code",
   "id": "22b2201ca139c0d0",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-09T21:34:22.997227Z",
     "start_time": "2025-09-09T21:34:22.931407Z"
    }
   },
   "source": [
    "# Test the value function without normalization\n",
    "print(f\"Output of the classifier: {classifier(test_sentence)}\")\n",
    "\n",
    "print(\n",
    "    f\"Value function for the full coalition: {value_function(full_coalition, tokenized_input=tokenized_input)[0]}\"\n",
    ")\n",
    "print(\n",
    "    f\"Value function for the empty coalition: {value_function(empty_coalition, tokenized_input=tokenized_input)[0]}\"\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of the classifier: [{'label': 'POSITIVE', 'score': 0.9655240178108215}]\n",
      "Value function for the full coalition: 0.9655240178108215\n",
      "Value function for the empty coalition: 0.5377568602561951\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "ae20674fc899a202",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "If we normalize the value function, the output of the value function for the empty coalition should be zero."
   ]
  },
  {
   "cell_type": "code",
   "id": "338e1ae439120652",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-09T21:34:24.049864Z",
     "start_time": "2025-09-09T21:34:24.012534Z"
    }
   },
   "source": [
    "# Test the value function with normalization\n",
    "normalization_value = float(value_function(empty_coalition, tokenized_input=tokenized_input)[0])\n",
    "print(\n",
    "    f\"Value function for the full coalition: {value_function(full_coalition, tokenized_input=tokenized_input, normalization_value=normalization_value)[0]}\"\n",
    ")\n",
    "print(\n",
    "    f\"Value function for the empty coalition: {value_function(empty_coalition, tokenized_input=tokenized_input, normalization_value=normalization_value)[0]}\"\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function for the full coalition: 0.42776715755462646\n",
      "Value function for the empty coalition: 0.0\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "7be865dbf772ea6c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "`shapiq` expects the game to be only dependent on the coalitions. For this we can write a small wrapper function:"
   ]
  },
  {
   "cell_type": "code",
   "id": "91e8b195226e1ecb",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-09T21:34:24.862412Z",
     "start_time": "2025-09-09T21:34:24.836616Z"
    }
   },
   "source": [
    "# Define the game function\n",
    "def game_fun(coalitions: np.ndarray[bool]) -> np.ndarray[float]:\n",
    "    \"\"\"Wrapper function for the value function.\n",
    "\n",
    "    Args:\n",
    "        coalitions: A numpy matrix of shape (n_coalitions, n_players).\n",
    "\n",
    "    Returns:\n",
    "        A vector of the value of the coalitions.\n",
    "    \"\"\"\n",
    "    return value_function(\n",
    "        coalitions, tokenized_input=tokenized_input, normalization_value=normalization_value\n",
    "    )\n",
    "\n",
    "\n",
    "# Test the game function\n",
    "print(f\"Game for the full coalition: {game_fun(full_coalition)[0]}\")\n",
    "print(f\"Game for the empty coalition: {game_fun(empty_coalition)[0]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game for the full coalition: 0.42776715755462646\n",
      "Game for the empty coalition: 0.0\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "5762eda66918ae03",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can use this callable already in `shapiq`, but we can also define it as a proper `Game` object, which comes with some additional functionality. Notice that the `value_function` function is now a method of the `SentimentClassificationGame` class and you do not have to worry about the normalization. This is done automatically by the `Game` class which also contains the `__call__` method meaning that this class is also callable."
   ]
  },
  {
   "cell_type": "code",
   "id": "ea94eb7697abad0d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-09T21:34:39.277472Z",
     "start_time": "2025-09-09T21:34:39.254110Z"
    }
   },
   "source": [
    "class SentimentClassificationGame(shapiq.Game):\n",
    "    \"\"\"The sentiment analysis classifier modeled as a cooperative game.\n",
    "\n",
    "    Args:\n",
    "        classifier: The sentiment analysis classifier.\n",
    "        tokenizer: The tokenizer of the classifier.\n",
    "        test_sentence: The sentence to be explained.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, classifier, tokenizer, test_sentence: str) -> None:\n",
    "        self.classifier = classifier\n",
    "        self.tokenizer = tokenizer\n",
    "        self.test_sentence = test_sentence\n",
    "        self.mask_token_id = tokenizer.mask_token_id\n",
    "        self.tokenized_input = np.asarray(tokenizer(test_sentence)[\"input_ids\"][1:-1])\n",
    "        self.n_players = len(self.tokenized_input)\n",
    "\n",
    "        empty_coalition = np.zeros((1, len(self.tokenized_input)), dtype=bool)\n",
    "        self.normalization_value = float(self.value_function(empty_coalition)[0])\n",
    "        super().__init__(n_players=self.n_players, normalization_value=self.normalization_value)\n",
    "\n",
    "    def value_function(self, coalitions: np.ndarray[bool]) -> np.ndarray[float]:\n",
    "        \"\"\"Computes the value of the coalitions.\n",
    "\n",
    "        Args:\n",
    "            coalitions: A numpy matrix of shape (n_coalitions, n_players).\n",
    "\n",
    "        Returns:\n",
    "            A vector of the value of the coalitions.\n",
    "        \"\"\"\n",
    "        texts = []\n",
    "        for coalition in coalitions:\n",
    "            tokenized_coalition = self.tokenized_input.copy()\n",
    "            # all tokens not in the coalition are set to mask_token_id\n",
    "            tokenized_coalition[~coalition] = self.mask_token_id\n",
    "            coalition_text = self.tokenizer.decode(tokenized_coalition)\n",
    "            texts.append(coalition_text)\n",
    "\n",
    "        # get the sentiment of the texts (call the model as defined above)\n",
    "        return self._model_call(texts)\n",
    "\n",
    "    def _model_call(self, input_texts: list[str]) -> np.ndarray[float]:\n",
    "        \"\"\"Calls the sentiment classification model with a list of texts.\n",
    "\n",
    "        Args:\n",
    "            input_texts: A list of input texts.\n",
    "\n",
    "        Returns:\n",
    "            A vector of the sentiment of the input texts.\n",
    "        \"\"\"\n",
    "        outputs = self.classifier(input_texts)\n",
    "        outputs = [\n",
    "            output[\"score\"] * 1 if output[\"label\"] == \"POSITIVE\" else output[\"score\"] * -1\n",
    "            for output in outputs\n",
    "        ]\n",
    "        return np.array(outputs, dtype=float)\n",
    "\n",
    "\n",
    "# Test SentimentClassificationGame\n",
    "game_class = SentimentClassificationGame(classifier, tokenizer, test_sentence)\n",
    "print(f\"Game for the full coalition: {game_class(full_coalition)[0]}\")\n",
    "print(f\"Game for the empty coalition: {game_class(empty_coalition)[0]}\")"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shapiq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mclass\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mSentimentClassificationGame\u001B[39;00m(\u001B[43mshapiq\u001B[49m.Game):\n\u001B[32m      2\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"The sentiment analysis classifier modeled as a cooperative game.\u001B[39;00m\n\u001B[32m      3\u001B[39m \n\u001B[32m      4\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m      7\u001B[39m \u001B[33;03m        test_sentence: The sentence to be explained.\u001B[39;00m\n\u001B[32m      8\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m     10\u001B[39m     \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, classifier, tokenizer, test_sentence: \u001B[38;5;28mstr\u001B[39m) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[31mNameError\u001B[39m: name 'shapiq' is not defined"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "5a100294487e50fc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Computing Shapley interactions\n",
    "We can now use the `game_fun` function or the `SentimentClassificationGame` class to compute the Shapley interactions with methods provided in `shapiq`."
   ]
  },
  {
   "cell_type": "code",
   "id": "f62adc49538c8a79",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-11T15:20:32.811002Z",
     "start_time": "2025-07-11T15:20:32.544518Z"
    }
   },
   "source": [
    "# Compute Shapley interactions with the ShapIQ approximator for the game function\n",
    "approximator = shapiq.KernelSHAPIQ(n=n_players, max_order=2, index=\"k-SII\")\n",
    "sii_values = approximator.approximate(budget=2**n_players, game=game_fun)\n",
    "sii_values.dict_values"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(): 0.0,\n",
       " (0,): 0.09466196418889895,\n",
       " (1,): 0.2519671876192255,\n",
       " (2,): 0.06853008486648426,\n",
       " (3,): 0.06228182818457484,\n",
       " (4,): 0.1502293735159068,\n",
       " (0, 1): -0.023901337999199194,\n",
       " (0, 2): -0.015578424181147861,\n",
       " (0, 3): 0.013715559286939426,\n",
       " (0, 4): -0.012585067760777176,\n",
       " (1, 2): 0.03777686295041179,\n",
       " (1, 3): -0.07309907222393518,\n",
       " (1, 4): -0.055708334461501,\n",
       " (2, 3): 0.01566102726098756,\n",
       " (2, 4): -0.06081690989708467,\n",
       " (3, 4): 0.022849774106212306}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "7641d33a850cdd16",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-11T15:20:33.049343Z",
     "start_time": "2025-07-11T15:20:32.822094Z"
    }
   },
   "source": [
    "# Compute Shapley interactions with the ShapIQ approximator for the game object\n",
    "approximator = shapiq.KernelSHAPIQ(n=game_class.n_players, max_order=2, index=\"k-SII\")\n",
    "sii_values = approximator.approximate(budget=2**game_class.n_players, game=game_class)\n",
    "sii_values.dict_values"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(): 0.0,\n",
       " (0,): 0.09466196418889895,\n",
       " (1,): 0.2519671876192255,\n",
       " (2,): 0.06853008486648426,\n",
       " (3,): 0.06228182818457484,\n",
       " (4,): 0.1502293735159068,\n",
       " (0, 1): -0.023901337999199194,\n",
       " (0, 2): -0.015578424181147861,\n",
       " (0, 3): 0.013715559286939426,\n",
       " (0, 4): -0.012585067760777176,\n",
       " (1, 2): 0.03777686295041179,\n",
       " (1, 3): -0.07309907222393518,\n",
       " (1, 4): -0.055708334461501,\n",
       " (2, 3): 0.01566102726098756,\n",
       " (2, 4): -0.06081690989708467,\n",
       " (3, 4): 0.022849774106212306}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "ef3641f671c8616b",
   "metadata": {},
   "source": [
    "Now let's say we want to do this for a much larger inputs. We can use the `shapiq.SPEX` approximator which is a sparse\n",
    "transform approximator. This approximator is much faster than the KernelSHAPIQ approximator when the number of\n",
    "players is large and can be used for larger inputs. Instead of computing all interactions it computes only the\n",
    "most important ones."
   ]
  },
  {
   "cell_type": "code",
   "id": "ce6bc4d47530fa2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T15:26:33.768828Z",
     "start_time": "2025-07-11T15:20:33.051503Z"
    }
   },
   "source": [
    "text = \"\"\"\n",
    "shapiq is a valuable Python library designed for Explainable AI (XAI), focusing specifically on approaches like\n",
    "Shapley values and their extensions. Its core strength lies in providing a unified framework to compute not only individual feature attributions but also sophisticated interaction indices (e.g., Shapley Interaction Index, Banzhaf Index). This allows users to gain deeper insights into how features collaborate or conflict within complex machine learning models, going beyond simple importance scores. A notable weakness stems from the inherent computational complexity of these game-theoretic measures. Calculating exact values, especially for higher-order interactions, is often infeasible, and even approximations can be computationally intensive and time-consuming, particularly for models with many features or large datasets. Despite this, shapiq remains a powerful tool for detailed model inspection.\n",
    "\"\"\"\n",
    "big_game = SentimentClassificationGame(\n",
    "    classifier=classifier, tokenizer=tokenizer, test_sentence=text\n",
    ")\n",
    "print(f\"There are a total of {big_game.n_players} players.\")\n",
    "# To speed up inference, run pipeline with gpu support. Takes ~10 minutes on Mac M1 with MPS.\n",
    "scalable_approximator = shapiq.SPEX(n=big_game.n_players, index=\"SII\")\n",
    "large_sii = scalable_approximator.approximate(budget=32000, game=big_game)\n",
    "print(f\"Game for the full coalition: {game_class(full_coalition)[0]}\")\n",
    "print(f\"Game for the empty coalition: {game_class(empty_coalition)[0]}\")\n",
    "interactions = list(large_sii.dict_values.items())\n",
    "interactions.sort(key=lambda x: abs(x[1]), reverse=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 175 players.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game for the full coalition: 0.47598451375961304\n",
      "Game for the empty coalition: 0.0\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "e81434a652831817",
   "metadata": {},
   "source": "`shapiq.SPEX` identifies interactions between the most sentiment-rich tokens in the paragraph (i.e. *powerful*, *valuable*, *weakness*)"
  },
  {
   "cell_type": "code",
   "id": "a09121f781d9be77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T15:26:33.792231Z",
     "start_time": "2025-07-11T15:26:33.790248Z"
    }
   },
   "source": [
    "for inter, value in interactions[:10]:\n",
    "    tokens = [big_game.tokenizer.decode(big_game.tokenized_input[idx]) for idx in inter]\n",
    "    print(f\"Tokens: {tokens}, Value: {value:.3f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['powerful'], Value: 0.040\n",
      "Tokens: ['valuable', 'powerful'], Value: -0.032\n",
      "Tokens: ['weakness', 'powerful'], Value: 0.026\n",
      "Tokens: ['valuable', 'weakness', 'powerful'], Value: -0.026\n",
      "Tokens: ['weakness'], Value: -0.024\n",
      "Tokens: ['valuable'], Value: 0.021\n",
      "Tokens: ['consuming', 'powerful'], Value: 0.016\n",
      "Tokens: ['insights', 'powerful'], Value: -0.016\n",
      "Tokens: ['valuable', 'weakness'], Value: 0.016\n",
      "Tokens: ['unified', 'powerful'], Value: -0.016\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "scalable_approximator = shapiq.ProxySPEX(n=big_game.n_players, index=\"SII\")\n",
    "large_sii = scalable_approximator.approximate(budget=3200, game=big_game)\n",
    "print(f\"Game for the full coalition: {game_class(full_coalition)[0]}\")\n",
    "print(f\"Game for the empty coalition: {game_class(empty_coalition)[0]}\")\n",
    "interactions = list(large_sii.dict_values.items())\n",
    "interactions.sort(key=lambda x: abs(x[1]), reverse=True)"
   ],
   "id": "b3c07949c87a2eb2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
